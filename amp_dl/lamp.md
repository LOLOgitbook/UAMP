# LAMP

ä¸€ã€  VAMP &#x20;

VAMP (Vector Approximate Message Passing) æ˜¯ä¸€ç§ **è¿­ä»£æ¨ç†ç®—æ³•**ï¼Œç”¨äºæ±‚è§£çº¿æ€§è§‚æµ‹æ¨¡å‹

y=Ax+n&#x20;

åœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼ŒVAMP è¿›è¡Œä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼ˆæ¨¡å—ï¼‰ï¼š

1. **çº¿æ€§æ¨¡å—**ï¼ˆLinear Estimationï¼‰\
   å‡è®¾ä¸€ä¸ªçº¿æ€§é«˜æ–¯æ¨¡å‹ï¼ŒåŸºäºå½“å‰ä¼°è®¡å¯¹ä¿¡å·è¿›è¡Œçº¿æ€§æ»¤æ³¢ï¼Œè¾“å‡º $$\mathbf{r}_t$$   ï¼Œä¼°è®¡è¯¯å·®æ–¹å·® $$\tau_t$$ ã€‚
2. **éçº¿æ€§æ¨¡å—**ï¼ˆDenoising Stepï¼‰\
   å‡è®¾ä¿¡å· x æœ‰æŸç§ç¨€ç–å…ˆéªŒï¼ˆå¦‚ Bernoulli-Gaussianï¼‰ï¼Œå¯¹    $$\mathbf{r}_t$$   åš MMSE æ¨æ–­ï¼š   $$\mathbf{x}_{t+1} = \mathbb{E}[\mathbf{x} | \mathbf{r}_t, \tau_t]$$  &#x20;

æ­¤æ—¶ç”¨åˆ°äº† **å·²çŸ¥çš„å…ˆéªŒ p(x)**ã€‚

#### ğŸ”§ äºŒã€Learned VAMP ç½‘ç»œï¼ˆæ·±åº¦å­¦ä¹ è§†è§’ï¼‰

Learned VAMP æ˜¯ä¸€ç§ **å°† VAMP ç»“æ„â€œå±•å¼€â€æˆç¥ç»ç½‘ç»œ** çš„æ–¹æ³•ï¼Œæ¯ä¸€å±‚å¯¹åº”ä¸€æ¬¡ VAMP è¿­ä»£ï¼š

1. ç½‘ç»œä¸­æ¯å±‚çš„ **çº¿æ€§æ“ä½œ**   $$\mathbf{W}_t \mathbf{y} + \ldots$$     æ˜¯å¯å­¦ä¹ å‚æ•°ï¼›
2.  éçº¿æ€§æ“ä½œç”±ç¥ç»ç½‘ç»œï¼ˆæˆ–å‚æ•°åŒ– shrinkage å‡½æ•°ï¼‰å®ç°ï¼Œå¦‚ï¼š   $$\mathbf{x}_{t+1} = \text{shrink}_{\boldsymbol{\theta}_t}(\mathbf{r}_t)$$   &#x20;

    &#x20;

    è¿™äº› shrink å‡½æ•°ä¸­å‚æ•° Î¸t æ˜¯**è®­ç»ƒå¾—åˆ°çš„**ã€‚

é€šè¿‡ **åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å±‚çš„å‚æ•° Wt,Î¸t åšæŸå¤±å‡½æ•°ï¼ˆå¦‚ NMSEï¼‰æœ€å°åŒ–ï¼Œä»è€Œå®ç°å­¦ä¹ ã€‚

#### âœ… ä¸‰ã€ä¸ºä»€ä¹ˆä¸¤è€…å­¦åˆ°ä¸€æ ·çš„ä¸œè¥¿ï¼Ÿ

* VAMP æ¨ç†ç®—æ³•ä¸­çš„éçº¿æ€§ä¼°è®¡å™¨å·²ç»æ˜¯ **æœ€ä¼˜çš„ MMSE æ¨æ–­å™¨**ï¼Œå‰ææ˜¯å…ˆéªŒå·²çŸ¥ï¼›
* åœ¨è®­ç»ƒä¿¡å·éµå¾ª Bernoulli-Gaussian ç­‰åˆ†å¸ƒæ—¶ï¼š
  * backpropagation å­¦ä¹ å‡ºçš„å‚æ•°å’Œ VAMP ä¸­æ¨å¯¼çš„å…¬å¼æ°å¥½ä¸€è‡´ï¼›
  * æ‰€ä»¥ Learned VAMP ç½‘ç»œå®é™…ä¸Šåœ¨**å­¦ä¹ è¿‡ç¨‹ä¸­è‡ªç„¶é€¼è¿‘äº† VAMP ç†è®ºç»™å‡ºçš„æœ€ä¼˜ç»“æ„**ã€‚

&#x20;

> **å½“è®­ç»ƒæ•°æ®æ»¡è¶³ VAMP å‡è®¾ï¼Œbackprop å­¦åˆ°çš„å°±æ˜¯ VAMP ä¸­çš„å…¬å¼ã€‚**

***

&#x20;

&#x20;

1.

```python
prob = problems.bernoulli_gaussian_trial 
```

ç”Ÿæˆä¸€ä¸ªç¨€ç–é«˜æ–¯ä¿¡å·æ¢å¤é—®é¢˜çš„æ•°æ®é›†ã€‚åŒ…å«éªŒè¯é›†ã€è®­ç»ƒé›†å’Œåˆå§‹ä¼°è®¡é›†åˆã€‚

é€šè¿‡bernoulli\_gaussianäº§ç”Ÿçš„

```python
    prob.name = 'Bernoulli-Gaussian, random A'
    prob.xval = ((np.random.uniform( 0,1,(N,L))<pnz) * np.random.normal(0,1,(N,L))).astype(np.float32) #ç”Ÿæˆ éªŒè¯é›†ç”¨çš„çœŸå®ä¿¡å·
    prob.yval = np.matmul(A,prob.xval) + np.random.normal(0,math.sqrt( noise_var ),(M,L))
    prob.xinit = ((np.random.uniform( 0,1,(N,L))<pnz) * np.random.normal(0,1,(N,L))).astype(np.float32) #ç”Ÿæˆ åˆå§‹ä¼°è®¡ç”¨çš„ä¿¡å·
    prob.yinit = np.matmul(A,prob.xinit) + np.random.normal(0,math.sqrt( noise_var ),(M,L)) # ç”Ÿæˆ åˆå§‹ä¼°è®¡ç”¨çš„è§‚æµ‹æ•°æ®
    prob.xgen_ = bernoulli_ * tf.random_normal( (N,L) ) # ç”Ÿæˆç¨€ç–çŸ©é˜µ
    prob.ygen_ = tf.matmul( A_,xgen_) + tf.random_normal( (M,L),stddev=math.sqrt( noise_var ) )
    prob.noise_var = noise_var 
```

2\.

```python
layers = networks.build_LAMP(prob,T=6,shrink='bg',untied=False)
```

&#x20;è¿”å›ï¼š(name,xhat\_,newvars)

ç”Ÿæˆetaå’Œ theta åˆå§‹

```python
eta= shrink_bgest # Bernoulli-Gaussian MMSE estimator
theta_init = (1,math.log(1/.1-1))
```

<figure><img src="../.gitbook/assets/Screenshot 2025-05-13 at 5.29.53â€¯pm.png" alt=""><figcaption></figcaption></figure>

```python
        (xhat_,dxdr_) = eta( rhat_ ,rvar_ , theta_ )
```



3. setup\_training

















