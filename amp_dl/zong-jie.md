# æ€»ç»“

#### ğŸ” ä¸ AMP/LAMP/VAMP çš„è”ç³»ï¼š

| ç®—æ³•                  | è¿­ä»£ç»“æ„             | éçº¿æ€§å‡½æ•° Î·t(â‹…)             | æ˜¯å¦æœ‰ Onsager æ ¡æ­£ |
| ------------------- | ---------------- | ----------------------- | -------------- |
| **AMP**             | (2.13) ç»“æ„        | å›ºå®š soft-threshold       | âœ… æœ‰            |
| **LAMP**            | åŒ AMP            | Î·t ä¸ºå¯è®­ç»ƒå‚æ•°               | âœ… æœ‰            |
| **VAMP**            | åŒé˜¶æ®µ AMP          | Î·t ã€Î·\~t å‡å¯è®­ç»ƒ           | âœ… æœ‰ï¼ˆåŒ Onsagerï¼‰ |
| **TISTA**           | ä¸ LISTA ç±»ä¼¼       | Î·t ä¸º soft + å¯å­¦ä¹ é—¨æ§       | âœ… æœ‰            |
| **DL-BP / DAMPNet** | BP æ ·å¼ message æ›´æ–° | æ·±åº¦ç½‘ç»œå­¦ä¹  message function | â“ ç»“æ„ä¾æ¨¡å‹ä¸åŒ      |

***

&#x20;

<figure><img src="../.gitbook/assets/Screenshot 2025-04-24 at 12.37.09â€¯pm.png" alt=""><figcaption></figcaption></figure>

### Training

some methods used to prevent overfitting concludes the section.

### Dataset

The training dataset is split in parts of equal size called batches before the start of the learning procedure. When the training phase starts. The algorithm iterates over epochs. For each epoch, a training step is repeated for every batch that compose the training dataset. A training step is divided into two stages:

&#x20;â€¢ forward pass computes the outputs $$\hat{x}^{1}$$, . . . ,  from the pairs input label $$\{  ( y^d, x^d )\}^D_{d=1}$$, where this time D stands for the size of the batch;&#x20;

â€¢ backward pass updates the values of  $$V_1$$, . . . ,  by computing their gradients with respect to the batch and then by applying gradient descent. T represents the number of layer of the network.

æ•°æ®é›†\
åœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰ï¼Œè®­ç»ƒæ•°æ®é›†ä¼šè¢«**åˆ’åˆ†ä¸ºå¤§å°ç›¸ç­‰çš„å¤šä¸ªæ‰¹æ¬¡ï¼ˆbatchesï¼‰**ã€‚è®­ç»ƒé˜¶æ®µå¼€å§‹åï¼Œç®—æ³•ä¼š**æŒ‰è½®æ¬¡ï¼ˆepochsï¼‰è¿›è¡Œè¿­ä»£**ã€‚åœ¨æ¯ä¸€è½®ï¼ˆepochï¼‰ä¸­ï¼Œé’ˆå¯¹æ¯ä¸€ä¸ª batch ä¼šé‡å¤è¿›è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤ã€‚æ¯ä¸€ä¸ªè®­ç»ƒæ­¥éª¤åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š

*   **å‰å‘ä¼ æ’­ï¼ˆforward passï¼‰**ï¼šæ ¹æ®è¾“å…¥æ ‡ç­¾å¯¹ $${(y^{(d)}, x^{(d)})}_{d=1}^{D}$$

    &#x20;  è®¡ç®—è¾“å‡º $$\hat{x}^{(1)}, \ldots, \hat{x}^{(D)}ï¼Œ$$ å…¶ä¸­ D æ˜¯å½“å‰ batch çš„å¤§å°ï¼›
* **åå‘ä¼ æ’­ï¼ˆbackward passï¼‰**ï¼šè®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºç½‘ç»œä¸­å‚æ•° $$V_1, \ldots, V_T$$  çš„æ¢¯åº¦ï¼Œå¹¶åº”ç”¨**æ¢¯åº¦ä¸‹é™**è¿›è¡Œå‚æ•°æ›´æ–°ã€‚å…¶ä¸­ TT è¡¨ç¤ºç½‘ç»œçš„å±‚æ•°ã€‚

æ¯ä¸ªæ ·æœ¬ç”±ä¸‰ç±»éšæœºæºç”Ÿæˆï¼š**ä¿¡å· x** ã€**ä¿¡é“çŸ©é˜µ H** å’Œ **å™ªå£° n** ï¼š

* x ä»è°ƒåˆ¶æ˜Ÿåº§ä¸­éšæœºã€å‡åŒ€é‡‡æ ·ï¼›
* H æ ¹æ®æ‰€é€‰ä¿¡é“æ¨¡å‹ç»“æ„è¿›è¡Œé‡‡æ ·ï¼›
* å™ªå£° n é€šè¿‡ SNR æ¨å¯¼å‡ºçš„æ ‡å‡†å·® Ïƒ ç”Ÿæˆï¼›
* æ¯ä¸ªæ ·æœ¬çš„ SNR åœ¨ \[18, 23] dB åŒºé—´å†…éšæœºé€‰å–ã€‚

å› æ­¤ï¼Œæ¯ä¸ª batch ä¸­çš„æ ·æœ¬æ˜¯å››å…ƒç»„å½¢å¼ï¼š

&#x20;$$\{(y^{(d)}, H^{(d)}, \sigma^{(d)}, x^{(d)})\}_{d=1}^{D}$$

å…¶ä¸­   $$x^{(d)}$$ æ˜¯å¾…é¢„æµ‹çš„æ ‡ç­¾ï¼ˆground truthï¼‰ã€‚

æœ¬è®ºæ–‡ä¸­çš„æ‰€æœ‰ç®—æ³•éƒ½æ˜¯ **ç¦»çº¿è®­ç»ƒçš„**ï¼ˆoffline trainingï¼‰ï¼š

* é¦–å…ˆåœ¨ **éšæœºé‡‡æ ·çš„ i.i.d. é«˜æ–¯ä¿¡é“çŸ©é˜µ** ä¸Šè®­ç»ƒï¼›
* ç„¶ååœ¨ **i.i.d. é«˜æ–¯ä¿¡é“çŸ©é˜µä¸ Kronecker ä¿¡é“æ¨¡å‹çŸ©é˜µ** ä¸Šæµ‹è¯•ï¼›
* è¿˜ä¼šåœ¨ Kronecker ä¿¡é“çŸ©é˜µä¸Šå†è®­ç»ƒä¸€æ¬¡ï¼Œå¹¶åœ¨ç›¸åŒæ¨¡å‹ç”Ÿæˆçš„ä¸åŒçŸ©é˜µä¸Šæµ‹è¯•ï¼Œä»¥éªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚

ç”Ÿæˆçš„è®­ç»ƒé›†ä¼šåˆ’åˆ†å‡º **25% ä½œä¸ºéªŒè¯é›†**ï¼ˆvalidation setï¼‰ï¼Œç”¨äº early stopping å’Œäº¤å‰éªŒè¯ï¼ˆcross-validationï¼‰ã€‚

æ¯ä¸€æ­¥è®­ç»ƒä¸­ï¼Œbatch éƒ½æ˜¯åŠ¨æ€éšæœºç”Ÿæˆçš„ï¼Œå› ä¸ºç”Ÿæˆ batch çš„å¼€é”€è¿œå°äºå‰å‘å’Œåå‘ä¼ æ’­ã€‚

æ‰€æœ‰æ·±åº¦å­¦ä¹ ç›¸å…³ç®—æ³•å‡åŸºäº **TensorFlow 2.0.0** å®ç°ã€è®­ç»ƒä¸æµ‹è¯•ã€‚ä¸åŒçš„ MIMO é…ç½®å’Œè°ƒåˆ¶é˜¶æ•°ä¸‹ä¼šä½¿ç”¨ä¸åŒçš„è®­ç»ƒè¿‡ç¨‹ã€‚

ä¸ºäº†**é˜²æ­¢è¿‡æ‹Ÿåˆ**ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š

* Early stoppingï¼ˆæ—©åœæ³•ï¼‰
* Dropoutï¼ˆéšæœºä¸¢å¼ƒï¼‰
* Cross-validationï¼ˆäº¤å‰éªŒè¯ï¼‰

è¿™äº›ç­–ç•¥åœ¨ä¸‹ä¸€èŠ‚ä¼šè¯¦ç»†è¯´æ˜ï¼Œå¹¶ä¸”å·²åº”ç”¨äºæ‰€æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„ç®—æ³•ã€‚

è®­ç»ƒå‚æ•°ï¼š

* è®­ç»ƒå‘¨æœŸï¼š2000 epochs
* ä¼˜åŒ–å™¨ï¼š**Adam**
* å­¦ä¹ ç‡ï¼š0.001
* æ¯ä¸ª batch å¤§å°ï¼š1000

Adam ä¼˜åŒ–å™¨ä¼šå¯¹æ¢¯åº¦åŠå…¶å¹³æ–¹è¿›è¡Œå¦‚ä¸‹è¿­ä»£æ›´æ–°ï¼š

&#x20;$$m_e = \beta_1 m_{e-1} + (1 - \beta_1) \nabla_W^{(k)} f_{f',e}$$ ,   $$v_e = \beta_2 v_{e-1} + (1 - \beta_2) (\nabla_W^{(k)} f_{f',e})^2$$

å…¶ä¸­ (1âˆ’Î²1) ã€(1âˆ’Î²2) åˆ†åˆ«ä»£è¡¨ä¸€é˜¶å’ŒäºŒé˜¶çŸ©ä¼°è®¡çš„å­¦ä¹ ç‡ã€‚
