# æ€»ç»“

#### ğŸ” ä¸ AMP/LAMP/VAMP çš„è”ç³»ï¼š

| ç®—æ³•                  | è¿­ä»£ç»“æ„             | éçº¿æ€§å‡½æ•° Î·t(â‹…)             | æ˜¯å¦æœ‰ Onsager æ ¡æ­£ |
| ------------------- | ---------------- | ----------------------- | -------------- |
| **AMP**             | (2.13) ç»“æ„        | å›ºå®š soft-threshold       | âœ… æœ‰            |
| **LAMP**            | åŒ AMP            | Î·t ä¸ºå¯è®­ç»ƒå‚æ•°               | âœ… æœ‰            |
| **VAMP**            | åŒé˜¶æ®µ AMP          | Î·t ã€Î·\~t å‡å¯è®­ç»ƒ           | âœ… æœ‰ï¼ˆåŒ Onsagerï¼‰ |
| **TISTA**           | ä¸ LISTA ç±»ä¼¼       | Î·t ä¸º soft + å¯å­¦ä¹ é—¨æ§       | âœ… æœ‰            |
| **DL-BP / DAMPNet** | BP æ ·å¼ message æ›´æ–° | æ·±åº¦ç½‘ç»œå­¦ä¹  message function | â“ ç»“æ„ä¾æ¨¡å‹ä¸åŒ      |

***

&#x20;

<figure><img src="../.gitbook/assets/Screenshot 2025-04-24 at 12.37.09â€¯pm.png" alt=""><figcaption></figcaption></figure>

### Training

some methods used to prevent overfitting concludes the section.

### Dataset

The training dataset is split in parts of equal size called batches before the start of the learning procedure. When the training phase starts. The algorithm iterates over epochs. For each epoch, a training step is repeated for every batch that compose the training dataset. A training step is divided into two stages:

&#x20;â€¢ forward pass computes the outputs $$\hat{x}^{1}$$, . . . ,  from the pairs input label $$\{  ( y^d, x^d )\}^D_{d=1}$$, where this time D stands for the size of the batch;&#x20;

â€¢ backward pass updates the values of  $$V_1$$, . . . ,  by computing their gradients with respect to the batch and then by applying gradient descent. T represents the number of layer of the network.

